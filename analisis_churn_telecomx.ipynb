{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TaliPaoHH/TelecomX-parte1/blob/main/analisis_churn_telecomx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIqsoW7Mbri0"
      },
      "source": [
        "# Análisis Completo de Churn - Telecom X\n",
        "**Proyecto integral de Data Science para identificar factores de cancelación**\n"
      ],
      "id": "PIqsoW7Mbri0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCbgBf-mbri5"
      },
      "outputs": [],
      "source": [
        "# === Configuración inicial e imports ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuración de visualización\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"Set2\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"ANÁLISIS COMPLETO DE CHURN - TELECOM X\")\n",
        "print(\"=\"*60)\n",
        "print(\"Proyecto de Data Science para retención de clientes\")\n",
        "print(\"=\"*60)\n"
      ],
      "id": "WCbgBf-mbri5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tv4AS7Mbri7"
      },
      "source": [
        "## 1. Extracción de datos desde API\n"
      ],
      "id": "2tv4AS7Mbri7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK6-kHTIbri7"
      },
      "outputs": [],
      "source": [
        "def cargar_datos_api():\n",
        "    \"\"\"\n",
        "    Función para cargar datos desde la API de GitHub con manejo robusto de errores\n",
        "    \"\"\"\n",
        "    try:\n",
        "        url = \"https://raw.githubusercontent.com/ingridcristh/challenge2-data-science-LATAM/main/TelecomX_Data.json\"\n",
        "        print(\"Conectando a la API...\")\n",
        "        response = requests.get(url, timeout=30)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        df = pd.DataFrame(data)\n",
        "        print(\"Datos cargados exitosamente\")\n",
        "        print(f\"Dimensiones: {df.shape[0]} filas x {df.shape[1]} columnas\")\n",
        "        return df\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error de conexión: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error inesperado: {e}\")\n",
        "        return None\n",
        "\n",
        "df_raw = cargar_datos_api()\n"
      ],
      "id": "wK6-kHTIbri7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT7Y1aCDbri8"
      },
      "source": [
        "## 2. Exploración inicial y comprensión del dataset\n"
      ],
      "id": "QT7Y1aCDbri8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9QeP41Ebri8"
      },
      "outputs": [],
      "source": [
        "if df_raw is not None:\n",
        "    print(\"\\nPASO 2: EXPLORACIÓN INICIAL DEL DATASET\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    def explorar_estructura_datos(df):\n",
        "        \"\"\"\n",
        "        Función para explorar la estructura básica del dataset\n",
        "        \"\"\"\n",
        "        print(\"INFORMACIÓN GENERAL DEL DATASET:\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        print(f\"Número de filas: {df.shape[0]:,}\")\n",
        "        print(f\"Número de columnas: {df.shape[1]}\")\n",
        "        try:\n",
        "            mem_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
        "        except Exception:\n",
        "            mem_mb = df.memory_usage().sum() / 1024**2\n",
        "        print(f\"Tamaño en memoria: {mem_mb:.2f} MB\")\n",
        "\n",
        "        print(\"\\nTIPOS DE DATOS POR COLUMNA:\")\n",
        "        print(\"=\" * 40)\n",
        "\n",
        "        tipo_datos = pd.DataFrame({\n",
        "            'Columna': df.columns,\n",
        "            'Tipo_Datos': df.dtypes.astype(str),\n",
        "            'Valores_Nulos': df.isnull().sum(),\n",
        "            'Porcentaje_Nulos': (df.isnull().sum() / len(df) * 100).round(2),\n",
        "            'Valores_Unicos': df.nunique(),\n",
        "            'Ejemplo_Valor': [df[col].dropna().iloc[0] if not df[col].dropna().empty else 'N/A' for col in df.columns]\n",
        "        })\n",
        "\n",
        "        print(tipo_datos.to_string(index=False))\n",
        "\n",
        "        numericas = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "        categoricas = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "        print(f\"\\nVariables numéricas ({len(numericas)}): {numericas}\")\n",
        "        print(f\"Variables categóricas ({len(categoricas)}): {categoricas}\")\n",
        "\n",
        "        return tipo_datos, numericas, categoricas\n",
        "\n",
        "    tipo_datos_info, cols_numericas, cols_categoricas = explorar_estructura_datos(df_raw)\n"
      ],
      "id": "w9QeP41Ebri8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEoOOEaIbri8"
      },
      "source": [
        "## 3. Identificación y manejo de inconsistencias\n"
      ],
      "id": "QEoOOEaIbri8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXTLQK5obri9"
      },
      "outputs": [],
      "source": [
        "def identificar_inconsistencias(df, cols_numericas=None, cols_categoricas=None):\n",
        "    \"\"\"\n",
        "    Función para identificar y reportar inconsistencias en los datos\n",
        "    \"\"\"\n",
        "    print(\"ANÁLISIS DE CALIDAD DE DATOS:\")\n",
        "    print(\"=\" * 35)\n",
        "\n",
        "    valores_nulos = df.isnull().sum()\n",
        "    if valores_nulos.sum() > 0:\n",
        "        print(\"Valores nulos encontrados:\")\n",
        "        for col, count in valores_nulos[valores_nulos > 0].items():\n",
        "            print(f\"  - {col}: {count} ({count/len(df)*100:.2f}%)\")\n",
        "    else:\n",
        "        print(\"No hay valores nulos\")\n",
        "\n",
        "    duplicados = df.duplicated().sum()\n",
        "    print(f\"\\nFilas duplicadas: {duplicados}\")\n",
        "\n",
        "    if cols_categoricas:\n",
        "        print(\"\\nANÁLISIS DE VARIABLES CATEGÓRICAS:\")\n",
        "        for col in cols_categoricas:\n",
        "            valores_unicos = df[col].dropna().unique()\n",
        "            print(f\"\\n  - {col}:\")\n",
        "            print(f\"    * Valores únicos: {len(valores_unicos)}\")\n",
        "            if len(valores_unicos) <= 10:\n",
        "                print(f\"    * Valores: {list(valores_unicos)}\")\n",
        "            else:\n",
        "                print(f\"    * Top 5: {list(df[col].value_counts().head().index)}\")\n",
        "\n",
        "    if cols_numericas:\n",
        "        print(\"\\nANÁLISIS DE VARIABLES NUMÉRICAS:\")\n",
        "        for col in cols_numericas:\n",
        "            col_data = df[col].dropna()\n",
        "            if len(col_data) == 0:\n",
        "                continue\n",
        "            print(f\"\\n  - {col}:\")\n",
        "            print(f\"    * Rango: {col_data.min()} - {col_data.max()}\")\n",
        "            print(f\"    * Media: {col_data.mean():.2f}\")\n",
        "            print(f\"    * Valores negativos: {(col_data < 0).sum()}\")\n",
        "            print(f\"    * Valores cero: {(col_data == 0).sum()}\")\n",
        "\n",
        "    return valores_nulos, duplicados\n",
        "\n",
        "nulos_info, duplicados_info = identificar_inconsistencias(df_raw, cols_numericas, cols_categoricas)\n"
      ],
      "id": "eXTLQK5obri9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGJWmzySbri9"
      },
      "source": [
        "## 4. Limpieza y transformación de datos\n"
      ],
      "id": "XGJWmzySbri9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLZwc3SWbri9"
      },
      "outputs": [],
      "source": [
        "def limpiar_y_transformar_datos(df, cols_categoricas):\n",
        "    \"\"\"\n",
        "    Función completa para limpiar y transformar los datos\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "    print(\"Aplicando transformaciones...\")\n",
        "\n",
        "    dups = df_clean.duplicated().sum()\n",
        "    if dups > 0:\n",
        "        df_clean = df_clean.drop_duplicates()\n",
        "        print(f\"Eliminados {dups} registros duplicados\")\n",
        "\n",
        "    for col in df_clean.columns:\n",
        "        if df_clean[col].isnull().sum() > 0:\n",
        "            if str(df_clean[col].dtype) in ['object', 'category']:\n",
        "                moda = df_clean[col].mode()\n",
        "                moda = moda.iloc[0] if not moda.empty else 'Unknown'\n",
        "                df_clean[col] = df_clean[col].fillna(moda)\n",
        "            else:\n",
        "                mediana = df_clean[col].median()\n",
        "                df_clean[col] = df_clean[col].fillna(mediana)\n",
        "\n",
        "    print(\"Estandarizando variables categóricas binarias (si aplica)...\")\n",
        "    binary_map = {\n",
        "        'Yes': 1, 'No': 0,\n",
        "        'Si': 1, 'Sí': 1, 'No': 0,\n",
        "        'TRUE': 1, 'FALSE': 0,\n",
        "        'True': 1, 'False': 0,\n",
        "        'Male': 1, 'Female': 0,\n",
        "        'Masculino': 1, 'Femenino': 0,\n",
        "        'M': 1, 'F': 0\n",
        "    }\n",
        "\n",
        "    for col in cols_categoricas:\n",
        "        vals = df_clean[col].dropna().unique()\n",
        "        if len(vals) == 2:\n",
        "            df_clean[col] = df_clean[col].replace(binary_map)\n",
        "\n",
        "    churn_col = None\n",
        "    for col in df_clean.columns:\n",
        "        low = col.lower()\n",
        "        if 'churn' in low or 'evasion' in low or 'cancel' in low:\n",
        "            churn_col = col\n",
        "            break\n",
        "\n",
        "    return df_clean, churn_col\n",
        "\n",
        "df_clean, churn_column = limpiar_y_transformar_datos(df_raw, cols_categoricas)\n"
      ],
      "id": "HLZwc3SWbri9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw2aRPNZbri-"
      },
      "source": [
        "## 5. Creación de columna: Cuentas_Diarias\n"
      ],
      "id": "yw2aRPNZbri-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfaSKDc5bri-"
      },
      "outputs": [],
      "source": [
        "def crear_cuentas_diarias(df):\n",
        "    \"\"\"Crea 'Cuentas_Diarias' a partir de una columna mensual si existe.\"\"\"\n",
        "    keywords = ['monthly', 'mensual', 'total', 'charge', 'cargo', 'bill', 'factura']\n",
        "    facturation_cols = [col for col in df.columns if any(k in col.lower() for k in keywords)]\n",
        "    print(f\"Columnas de facturación identificadas: {facturation_cols}\")\n",
        "    if facturation_cols:\n",
        "        monthly_col = facturation_cols[0]\n",
        "        df[monthly_col] = pd.to_numeric(df[monthly_col], errors='coerce')\n",
        "        df['Cuentas_Diarias'] = (df[monthly_col] / 30.0).fillna(0)\n",
        "        print(f\"Columna 'Cuentas_Diarias' creada basada en {monthly_col}\")\n",
        "        print(\"Estadísticas Cuentas_Diarias:\")\n",
        "        print(f\"  - Media: ${df['Cuentas_Diarias'].mean():.2f}\")\n",
        "        print(f\"  - Mediana: ${df['Cuentas_Diarias'].median():.2f}\")\n",
        "        print(f\"  - Min-Max: ${df['Cuentas_Diarias'].min():.2f} - ${df['Cuentas_Diarias'].max():.2f}\")\n",
        "    else:\n",
        "        print(\"No se encontraron columnas de facturación para calcular cuentas diarias\")\n",
        "    return df\n",
        "\n",
        "df_clean = crear_cuentas_diarias(df_clean)\n"
      ],
      "id": "TfaSKDc5bri-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDR8u_tVbri-"
      },
      "source": [
        "## 6. Análisis descriptivo\n"
      ],
      "id": "YDR8u_tVbri-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldQVoDzxbri-"
      },
      "outputs": [],
      "source": [
        "def analisis_descriptivo_completo(df, churn_col, cols_numericas, cols_categoricas):\n",
        "    print(\"ESTADÍSTICAS DESCRIPTIVAS GENERALES:\")\n",
        "    print(\"=\" * 45)\n",
        "    if cols_numericas:\n",
        "        desc_stats = df[cols_numericas].describe().round(2)\n",
        "        print(desc_stats)\n",
        "    else:\n",
        "        desc_stats = pd.DataFrame()\n",
        "        print(\"No hay columnas numéricas para describir.\")\n",
        "\n",
        "    if churn_col and churn_col in df.columns:\n",
        "        print(f\"\\nANÁLISIS ESPECÍFICO DE CHURN ({churn_col}):\")\n",
        "        print(\"=\" * 45)\n",
        "        churn_dist = df[churn_col].value_counts(dropna=False)\n",
        "        churn_pct = df[churn_col].value_counts(normalize=True, dropna=False) * 100\n",
        "        print(\"Distribución de Churn:\")\n",
        "        for val in churn_dist.index:\n",
        "            cantidad = churn_dist.loc[val]\n",
        "            porcentaje = churn_pct.loc[val]\n",
        "            print(f\"  - {val}: {cantidad:,} clientes ({porcentaje:.2f}%)\")\n",
        "        try:\n",
        "            if pd.api.types.is_numeric_dtype(df[churn_col]):\n",
        "                tasa_churn = pd.to_numeric(df[churn_col], errors='coerce').mean()\n",
        "            else:\n",
        "                vals = df[churn_col].dropna().unique()\n",
        "                if len(vals) == 2:\n",
        "                    yes_alias = {'yes', 'sí', 'si', '1', 'true'}\n",
        "                    val_churn = None\n",
        "                    for v in vals:\n",
        "                        if str(v).strip().lower() in yes_alias:\n",
        "                            val_churn = v\n",
        "                            break\n",
        "                    if val_churn is None:\n",
        "                        val_churn = vals[0]\n",
        "                    tasa_churn = (df[churn_col] == val_churn).mean()\n",
        "                else:\n",
        "                    tasa_churn = np.nan\n",
        "        except Exception:\n",
        "            tasa_churn = np.nan\n",
        "        print(f\"\\nTasa de Churn: {tasa_churn:.2%}\" if pd.notna(tasa_churn) else \"\\nTasa de Churn: no determinable\")\n",
        "\n",
        "        print(\"\\nANÁLISIS POR VARIABLES CATEGÓRICAS:\")\n",
        "        for col in cols_categoricas:\n",
        "            if col != churn_col and df[col].nunique() <= 10:\n",
        "                print(f\"\\n  - Churn por {col}:\")\n",
        "                try:\n",
        "                    crosstab = pd.crosstab(df[col], df[churn_col], normalize='index') * 100\n",
        "                    for idx in crosstab.index:\n",
        "                        churn_rate_segment = crosstab.loc[idx].max()\n",
        "                        print(f\"    * {idx}: {churn_rate_segment:.1f}% de churn (máx. por categoría)\")\n",
        "                except Exception as e:\n",
        "                    print(f\"    * No se pudo calcular: {e}\")\n",
        "    return desc_stats\n",
        "\n",
        "estadisticas_desc = analisis_descriptivo_completo(df_clean, churn_column, cols_numericas, cols_categoricas)\n"
      ],
      "id": "ldQVoDzxbri-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zfpAwgQbri-"
      },
      "source": [
        "## 7. Visualizaciones avanzadas (matplotlib / seaborn)\n"
      ],
      "id": "8zfpAwgQbri-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABGtkcJ7bri_"
      },
      "outputs": [],
      "source": [
        "def crear_visualizaciones_avanzadas(df, churn_col, cols_numericas, cols_categoricas):\n",
        "    fig = plt.figure(figsize=(20, 16))\n",
        "    # 1. Distribución de Churn\n",
        "    plt.subplot(3, 3, 1)\n",
        "    if churn_col and churn_col in df.columns:\n",
        "        churn_counts = df[churn_col].value_counts(dropna=False)\n",
        "        labels = [str(x) for x in churn_counts.index]\n",
        "        plt.pie(churn_counts.values, labels=labels, autopct='%1.1f%%', startangle=90)\n",
        "        plt.title(f'Distribución de {churn_col}', fontweight='bold')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Columna de Churn no identificada', ha='center', va='center')\n",
        "        plt.title('Distribución de Churn')\n",
        "    # 2. Histograma de la primera variable numérica\n",
        "    plt.subplot(3, 3, 2)\n",
        "    if len(cols_numericas) > 0:\n",
        "        primera_numerica = cols_numericas[0]\n",
        "        try:\n",
        "            plt.hist(pd.to_numeric(df[primera_numerica], errors='coerce').dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
        "            plt.title(f'Distribución de {primera_numerica}')\n",
        "            plt.xlabel(primera_numerica)\n",
        "            plt.ylabel('Frecuencia')\n",
        "        except Exception as e:\n",
        "            plt.text(0.5, 0.5, f'No disponible: {e}', ha='center')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Sin variables numéricas', ha='center')\n",
        "    # 3. Boxplot comparativo si hay churn\n",
        "    plt.subplot(3, 3, 3)\n",
        "    if churn_col and len(cols_numericas) > 0 and churn_col in df.columns:\n",
        "        try:\n",
        "            sns.boxplot(data=df, x=churn_col, y=cols_numericas[0])\n",
        "            plt.title(f'{cols_numericas[0]} por {churn_col}')\n",
        "        except Exception as e:\n",
        "            plt.text(0.5, 0.5, f'No disponible: {e}', ha='center')\n",
        "    # 4. Matriz de correlación\n",
        "    plt.subplot(3, 3, 4)\n",
        "    num_cols = [c for c in cols_numericas if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    if len(num_cols) >= 2:\n",
        "        try:\n",
        "            corr_matrix = df[num_cols].corr(numeric_only=True)\n",
        "            sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, square=True, cbar_kws={'shrink': 0.8})\n",
        "            plt.title('Matriz de Correlación')\n",
        "        except Exception as e:\n",
        "            plt.text(0.5, 0.5, f'No disponible: {e}', ha='center')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Variables numéricas insuficientes', ha='center')\n",
        "    # 5. Churn por primera categórica\n",
        "    plt.subplot(3, 3, 5)\n",
        "    if churn_col and len(cols_categoricas) > 0:\n",
        "        primera_cat = cols_categoricas[0] if cols_categoricas[0] != churn_col else (cols_categoricas[1] if len(cols_categoricas) > 1 else None)\n",
        "        if primera_cat is not None and churn_col in df.columns:\n",
        "            try:\n",
        "                pd.crosstab(df[primera_cat], df[churn_col]).plot(kind='bar', stacked=True, ax=plt.gca())\n",
        "                plt.title(f'Churn por {primera_cat}')\n",
        "                plt.xticks(rotation=45)\n",
        "            except Exception as e:\n",
        "                plt.text(0.5, 0.5, f'No disponible: {e}', ha='center')\n",
        "        else:\n",
        "            plt.text(0.5, 0.5, 'Categóricas insuficientes', ha='center')\n",
        "    # 6. Distribución de Cuentas_Diarias\n",
        "    plt.subplot(3, 3, 6)\n",
        "    if 'Cuentas_Diarias' in df.columns:\n",
        "        try:\n",
        "            plt.hist(pd.to_numeric(df['Cuentas_Diarias'], errors='coerce').dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
        "            plt.title('Distribución de Cuentas Diarias')\n",
        "            plt.xlabel('Cuentas Diarias ($)')\n",
        "            plt.ylabel('Frecuencia')\n",
        "        except Exception as e:\n",
        "            plt.text(0.5, 0.5, f'No disponible: {e}', ha='center')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Sin Cuentas_Diarias', ha='center')\n",
        "    # 7. Tasa de churn por rangos de cuentas\n",
        "    plt.subplot(3, 3, 7)\n",
        "    if 'Cuentas_Diarias' in df.columns and churn_col and churn_col in df.columns:\n",
        "        try:\n",
        "            tmp = df.copy()\n",
        "            tmp['Rango_Cuentas'] = pd.cut(pd.to_numeric(tmp['Cuentas_Diarias'], errors='coerce'), bins=5, labels=['Muy Bajo', 'Bajo', 'Medio', 'Alto', 'Muy Alto'])\n",
        "            pd.crosstab(tmp['Rango_Cuentas'], tmp[churn_col], normalize='index').plot(kind='bar', ax=plt.gca())\n",
        "            plt.title('Tasa de Churn por Rango de Cuentas')\n",
        "            plt.xticks(rotation=45)\n",
        "        except Exception as e:\n",
        "            plt.text(0.5, 0.5, f'No disponible: {e}', ha='center')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'Requiere Churn y Cuentas_Diarias', ha='center')\n",
        "    # 8. Top categorías\n",
        "    plt.subplot(3, 3, 8)\n",
        "    if len(cols_categoricas) > 0:\n",
        "        cat_col = cols_categoricas[0] if (churn_col is None or cols_categoricas[0] != churn_col) else (cols_categoricas[1] if len(cols_categoricas) > 1 else None)\n",
        "        if cat_col:\n",
        "            try:\n",
        "                top_cats = df[cat_col].value_counts().head(8)\n",
        "                top_cats.plot(kind='barh', ax=plt.gca())\n",
        "                plt.title(f'Top Categorías - {cat_col}')\n",
        "            except Exception as e:\n",
        "                plt.text(0.5, 0.5, f'No disponible: {e}', ha='center')\n",
        "        else:\n",
        "            plt.text(0.5, 0.5, 'Categóricas insuficientes', ha='center')\n",
        "    # 9. Evolución temporal\n",
        "    plt.subplot(3, 3, 9)\n",
        "    time_cols = [col for col in df.columns if any(k in col.lower() for k in ['date', 'time', 'mes', 'fecha'])]\n",
        "    if time_cols:\n",
        "        time_col = time_cols[0]\n",
        "        try:\n",
        "            if not np.issubdtype(df[time_col].dtype, np.datetime64):\n",
        "                df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
        "            ts = df.dropna(subset=[time_col]).copy()\n",
        "            ts = ts.sort_values(time_col)\n",
        "            if churn_col and churn_col in df.columns and pd.api.types.is_numeric_dtype(df[churn_col]):\n",
        "                time_churn = ts.groupby(time_col)[churn_col].mean()\n",
        "                time_churn.plot(kind='line', marker='o', ax=plt.gca())\n",
        "                plt.title(f'Evolución de Churn por {time_col}')\n",
        "            else:\n",
        "                ts[time_col].value_counts().sort_index().plot(kind='line', marker='o', ax=plt.gca())\n",
        "                plt.title(f'Distribución temporal - {time_col}')\n",
        "        except Exception as e:\n",
        "            plt.text(0.5, 0.5, f'No disponible: {e}', ha='center')\n",
        "    else:\n",
        "        plt.text(0.5, 0.5, 'No hay columnas temporales', ha='center')\n",
        "        plt.title('Análisis Temporal')\n",
        "    plt.tight_layout()\n",
        "    plt.suptitle('ANÁLISIS VISUAL COMPLETO - TELECOM X', fontsize=16, fontweight='bold', y=1.02)\n",
        "    plt.show()\n",
        "    return fig\n",
        "\n",
        "_ = crear_visualizaciones_avanzadas(df_clean, churn_column, cols_numericas, cols_categoricas)\n"
      ],
      "id": "ABGtkcJ7bri_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGzo_BS3bri_"
      },
      "source": [
        "## 8. Análisis de correlación avanzado\n"
      ],
      "id": "rGzo_BS3bri_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nihpV1bpbri_"
      },
      "outputs": [],
      "source": [
        "def analisis_correlacion_avanzado(df, churn_col, cols_numericas):\n",
        "    print(\"ANÁLISIS DE CORRELACIONES:\")\n",
        "    print(\"=\" * 35)\n",
        "    correlaciones_fuertes = []\n",
        "    num_cols = [c for c in cols_numericas if pd.api.types.is_numeric_dtype(df[c])]\n",
        "    if len(num_cols) >= 2:\n",
        "        corr_matrix = df[num_cols].corr(numeric_only=True)\n",
        "        for i in range(len(corr_matrix.columns)):\n",
        "            for j in range(i+1, len(corr_matrix.columns)):\n",
        "                var1, var2 = corr_matrix.columns[i], corr_matrix.columns[j]\n",
        "                corr_val = corr_matrix.iloc[i, j]\n",
        "                if pd.notna(corr_val) and abs(corr_val) > 0.3:\n",
        "                    correlaciones_fuertes.append((var1, var2, float(corr_val)))\n",
        "        correlaciones_fuertes.sort(key=lambda x: abs(x[2]), reverse=True)\n",
        "        print(\"Top correlaciones (|r| > 0.3):\")\n",
        "        for i, (var1, var2, corr) in enumerate(correlaciones_fuertes[:10], 1):\n",
        "            print(f\"  {i}. {var1} ↔ {var2}: {corr:.3f}\")\n",
        "    if churn_col and churn_col in df.columns and num_cols:\n",
        "        try:\n",
        "            if pd.api.types.is_numeric_dtype(df[churn_col]):\n",
        "                churn_correlations = df[num_cols].corrwith(df[churn_col]).sort_values(key=lambda s: s.abs(), ascending=False)\n",
        "                print(\"\\nVariables más correlacionadas con churn:\")\n",
        "                for var, corr in churn_correlations.head(10).items():\n",
        "                    if var != churn_col and pd.notna(corr):\n",
        "                        print(f\"  - {var}: {corr:.3f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"No fue posible calcular correlación con churn: {e}\")\n",
        "    return correlaciones_fuertes\n",
        "\n",
        "correlaciones_info = analisis_correlacion_avanzado(df_clean, churn_column, cols_numericas)\n"
      ],
      "id": "nihpV1bpbri_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHX7DnZ4bri_"
      },
      "source": [
        "## 9. Insights y conclusiones avanzadas\n"
      ],
      "id": "HHX7DnZ4bri_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5L5WwdQLbrjA"
      },
      "outputs": [],
      "source": [
        "def generar_insights_avanzados(df, churn_col, cols_categoricas):\n",
        "    insights = []\n",
        "    completeness = (1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
        "    insights.append(f\"Calidad de datos: {completeness:.1f}% completo, {len(df)} clientes analizados\")\n",
        "    churn_rate = np.nan\n",
        "    if churn_col and churn_col in df.columns:\n",
        "        try:\n",
        "            if pd.api.types.is_numeric_dtype(df[churn_col]):\n",
        "                churn_rate = df[churn_col].mean()\n",
        "            else:\n",
        "                vals = df[churn_col].dropna().unique()\n",
        "                if len(vals) == 2:\n",
        "                    yes_alias = {'yes', 'sí', 'si', '1', 'true'}\n",
        "                    val_churn = None\n",
        "                    for v in vals:\n",
        "                        if str(v).strip().lower() in yes_alias:\n",
        "                            val_churn = v\n",
        "                            break\n",
        "                    if val_churn is None:\n",
        "                        val_churn = vals[0]\n",
        "                    churn_rate = (df[churn_col] == val_churn).mean()\n",
        "        except Exception:\n",
        "            churn_rate = np.nan\n",
        "    if pd.notna(churn_rate):\n",
        "        sever = \"CRÍTICA\" if churn_rate > 0.3 else (\"ALTA\" if churn_rate > 0.2 else \"MODERADA\")\n",
        "        insights.append(f\"Tasa de churn actual: {churn_rate:.1%} - {sever}\")\n",
        "    gender_cols = [col for col in cols_categoricas if any(word in col.lower() for word in ['gender', 'genero', 'sexo'])]\n",
        "    if gender_cols and churn_col and churn_col in df.columns and pd.api.types.is_numeric_dtype(df[churn_col]):\n",
        "        gender_col = gender_cols[0]\n",
        "        try:\n",
        "            gender_churn = df.groupby(gender_col)[churn_col].mean()\n",
        "            if len(gender_churn) >= 2:\n",
        "                diff = float(gender_churn.max() - gender_churn.min())\n",
        "                if diff > 0.05:\n",
        "                    max_gender = str(gender_churn.idxmax())\n",
        "                    insights.append(f\"Diferencia por género: {max_gender} presenta {diff:.1%} más churn que el otro grupo\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    if 'Cuentas_Diarias' in df.columns and churn_col and churn_col in df.columns and pd.api.types.is_numeric_dtype(df[churn_col]):\n",
        "        try:\n",
        "            churn_daily = df.loc[df[churn_col] == 1, 'Cuentas_Diarias'].mean()\n",
        "            no_churn_daily = df.loc[df[churn_col] == 0, 'Cuentas_Diarias'].mean()\n",
        "            diff_daily = churn_daily - no_churn_daily\n",
        "            if pd.notna(diff_daily):\n",
        "                insights.append(f\"Clientes que cancelan gastan ${abs(diff_daily):.2f} {'menos' if diff_daily < 0 else 'más'} diariamente\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    num_count = len(df.select_dtypes(include=[np.number]).columns)\n",
        "    if num_count >= 3:\n",
        "        insights.append(f\"Dataset con {num_count} variables numéricas útiles para análisis predictivo\")\n",
        "    if churn_col and churn_col in df.columns and 'Cuentas_Diarias' in df.columns and pd.api.types.is_numeric_dtype(df[churn_col]):\n",
        "        total_churn_customers = int((df[churn_col] == 1).sum())\n",
        "        avg_revenue_lost = float(df.loc[df[churn_col] == 1, 'Cuentas_Diarias'].mean() * 365)\n",
        "        if not np.isnan(avg_revenue_lost):\n",
        "            total_revenue_at_risk = total_churn_customers * avg_revenue_lost\n",
        "            insights.append(f\"Ingresos anuales en riesgo (estimado): ${total_revenue_at_risk:,.0f}\")\n",
        "    return insights\n",
        "\n",
        "insights_avanzados = generar_insights_avanzados(df_clean, churn_column, cols_categoricas)\n",
        "print(\"\\nINSIGHTS PRINCIPALES:\")\n",
        "for i, ins in enumerate(insights_avanzados, 1):\n",
        "    print(f\"{i}. {ins}\")\n"
      ],
      "id": "5L5WwdQLbrjA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeNxwKUTbrjA"
      },
      "source": [
        "## 10. Informe ejecutivo final\n"
      ],
      "id": "HeNxwKUTbrjA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3ZIKhQMbrjA"
      },
      "outputs": [],
      "source": [
        "def generar_informe_ejecutivo(df, churn_col, insights, cols_numericas, cols_categoricas):\n",
        "    print(\"TELECOM X - ANÁLISIS DE CHURN DE CLIENTES\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Fecha del análisis: {datetime.now().strftime('%d/%m/%Y %H:%M')}\")\n",
        "    print(f\"Analista: Sistema de Data Science\")\n",
        "    print(f\"Dataset: {len(df)} clientes, {len(df.columns)} variables\")\n",
        "    print(\"\\nRESUMEN EJECUTIVO:\")\n",
        "    print(\"-\" * 25)\n",
        "    for i, insight in enumerate(insights, 1):\n",
        "        print(f\"{i}. {insight}\")\n",
        "    print(\"\\nPRINCIPALES HALLAZGOS:\")\n",
        "    print(\"-\" * 30)\n",
        "    hallazgos = []\n",
        "    missing_percentage = (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100\n",
        "    if missing_percentage < 5:\n",
        "        hallazgos.append(\"Excelente calidad de datos - menos del 5% de valores faltantes\")\n",
        "    elif missing_percentage < 15:\n",
        "        hallazgos.append(\"Calidad de datos aceptable - requiere limpieza menor\")\n",
        "    else:\n",
        "        hallazgos.append(\"Calidad de datos comprometida - requiere limpieza extensiva\")\n",
        "    if churn_col:\n",
        "        unique_values = df[churn_col].nunique()\n",
        "        if unique_values == 2:\n",
        "            hallazgos.append(\"Variable de churn identificada correctamente - análisis binario posible\")\n",
        "        else:\n",
        "            hallazgos.append(\"Variable de churn con múltiples categorías - requiere recodificación\")\n",
        "    if len(cols_numericas) >= 5:\n",
        "        hallazgos.append(\"Dataset rico en variables numéricas - adecuado para modelos predictivos\")\n",
        "    categorical_diversity = sum(df[col].nunique() for col in cols_categoricas if df[col].nunique() <= 10) if cols_categoricas else 0\n",
        "    if categorical_diversity >= 20:\n",
        "        hallazgos.append(\"Múltiples dimensiones de segmentación disponibles\")\n",
        "    for i, h in enumerate(hallazgos, 1):\n",
        "        print(f\"{i}. {h}\")\n",
        "    print(\"\\nRECOMENDACIONES ESTRATÉGICAS:\")\n",
        "    print(\"-\" * 35)\n",
        "    recomendaciones = [\n",
        "        \"Implementar modelo predictivo de churn (Random Forest o XGBoost)\",\n",
        "        \"Desarrollar sistema de alertas tempranas para clientes en riesgo\",\n",
        "        \"Crear campañas de retención personalizadas por segmentos\",\n",
        "        \"Analizar el lifetime value para priorizar esfuerzos de retención\",\n",
        "        \"Establecer programa de contact center proactivo para clientes de alto riesgo\",\n",
        "        \"Diseñar ofertas y promociones basadas en patrones de comportamiento\",\n",
        "        \"Implementar dashboard ejecutivo para monitoreo continuo de KPIs\"\n",
        "    ]\n",
        "    for i, rec in enumerate(recomendaciones, 1):\n",
        "        print(f\"{i}. {rec}\")\n",
        "    print(\"\\nPRÓXIMOS PASOS TÉCNICOS:\")\n",
        "    print(\"-\" * 30)\n",
        "    proximos_pasos = [\n",
        "        \"Feature engineering: crear variables derivadas y ratios\",\n",
        "        \"Experimentación con algoritmos de ML: RF, XGB, Redes Neuronales\",\n",
        "        \"Análisis de cohortes para entender comportamiento temporal\",\n",
        "        \"Implementación de métricas de negocio: CLV, CAC, Payback period\",\n",
        "        \"Desarrollo de visualizaciones interactivas con Plotly/Dash\",\n",
        "        \"Deploy del modelo en producción (MLflow/Docker)\",\n",
        "        \"A/B testing para validar estrategias de retención\"\n",
        "    ]\n",
        "    for i, paso in enumerate(proximos_pasos, 1):\n",
        "        print(f\"{i}. {paso}\")\n",
        "    return hallazgos, recomendaciones, proximos_pasos\n",
        "\n",
        "hallazgos_final, recomendaciones_final, pasos_final = generar_informe_ejecutivo(df_clean, churn_column, insights_avanzados, cols_numericas, cols_categoricas)\n"
      ],
      "id": "E3ZIKhQMbrjA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr1iRFQdbrjA"
      },
      "source": [
        "## 11. Métricas de negocio y ROI\n"
      ],
      "id": "xr1iRFQdbrjA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrjdRDIrbrjA"
      },
      "outputs": [],
      "source": [
        "def calcular_impacto_negocio(df, churn_col):\n",
        "    print(\"ANÁLISIS DE IMPACTO ECONÓMICO:\")\n",
        "    print(\"=\" * 35)\n",
        "    if churn_col and 'Cuentas_Diarias' in df.columns:\n",
        "        total_clientes = len(df)\n",
        "        if pd.api.types.is_numeric_dtype(df[churn_col]):\n",
        "            clientes_churn = int((df[churn_col] == 1).sum())\n",
        "            clientes_activos = int((df[churn_col] == 0).sum())\n",
        "            tasa_churn = float(df[churn_col].mean())\n",
        "            revenue_perdido = float(df.loc[df[churn_col] == 1, 'Cuentas_Diarias'].sum() * 365)\n",
        "            revenue_activo = float(df.loc[df[churn_col] == 0, 'Cuentas_Diarias'].sum() * 365)\n",
        "            revenue_promedio_churn = float(df.loc[df[churn_col] == 1, 'Cuentas_Diarias'].mean() * 365)\n",
        "            revenue_promedio_activo = float(df.loc[df[churn_col] == 0, 'Cuentas_Diarias'].mean() * 365)\n",
        "        else:\n",
        "            vals = df[churn_col].dropna().unique()\n",
        "            val_churn = vals[0] if len(vals) == 1 else ('Yes' if 'Yes' in vals else vals[0])\n",
        "            clientes_churn = int((df[churn_col] == val_churn).sum())\n",
        "            clientes_activos = int((df[churn_col] != val_churn).sum())\n",
        "            tasa_churn = float((df[churn_col] == val_churn).mean())\n",
        "            revenue_perdido = float(df.loc[df[churn_col] == val_churn, 'Cuentas_Diarias'].sum() * 365)\n",
        "            revenue_activo = float(df.loc[df[churn_col] != val_churn, 'Cuentas_Diarias'].sum() * 365)\n",
        "            revenue_promedio_churn = float(df.loc[df[churn_col] == val_churn, 'Cuentas_Diarias'].mean() * 365)\n",
        "            revenue_promedio_activo = float(df.loc[df[churn_col] != val_churn, 'Cuentas_Diarias'].mean() * 365)\n",
        "        total_revenue = revenue_perdido + revenue_activo\n",
        "        print(f\"Total de clientes: {total_clientes:,}\")\n",
        "        print(f\"Clientes activos: {clientes_activos:,} ({(clientes_activos/total_clientes)*100:.1f}%)\")\n",
        "        print(f\"Clientes perdidos: {clientes_churn:,} ({tasa_churn*100:.1f}%)\")\n",
        "        print(f\"\\nRevenue total anual: ${total_revenue:,.2f}\")\n",
        "        print(f\"Revenue perdido anual: ${revenue_perdido:,.2f}\")\n",
        "        print(f\"Revenue activo anual: ${revenue_activo:,.2f}\")\n",
        "        print(\"\\nRevenue promedio por cliente:\")\n",
        "        print(f\"  - Clientes que se van: ${revenue_promedio_churn:,.2f}/año\")\n",
        "        print(f\"  - Clientes que se quedan: ${revenue_promedio_activo:,.2f}/año\")\n",
        "        if tasa_churn > 0 and not np.isnan(revenue_promedio_churn):\n",
        "            reduccion_churn_25 = clientes_churn * 0.25\n",
        "            revenue_recuperable_25 = reduccion_churn_25 * revenue_promedio_churn\n",
        "            reduccion_churn_50 = clientes_churn * 0.50\n",
        "            revenue_recuperable_50 = reduccion_churn_50 * revenue_promedio_churn\n",
        "            print(\"\\nPOTENCIAL DE MEJORA:\")\n",
        "            print(f\"  - Reduciendo churn 25%: ${revenue_recuperable_25:,.2f} adicionales/año\")\n",
        "            print(f\"  - Reduciendo churn 50%: ${revenue_recuperable_50:,.2f} adicionales/año\")\n",
        "            costo_retencion_cliente = revenue_promedio_churn * 0.15\n",
        "            inversion_programa_25 = reduccion_churn_25 * costo_retencion_cliente\n",
        "            if inversion_programa_25 > 0:\n",
        "                roi_25 = (revenue_recuperable_25 - inversion_programa_25) / inversion_programa_25 * 100\n",
        "                print(\"\\nROI ESTIMADO (reducción 25% churn):\")\n",
        "                print(f\"  - Inversión estimada: ${inversion_programa_25:,.2f}\")\n",
        "                print(f\"  - ROI proyectado: {roi_25:.1f}%\")\n",
        "        return {\n",
        "            'total_clientes': total_clientes,\n",
        "            'clientes_churn': clientes_churn,\n",
        "            'tasa_churn': tasa_churn,\n",
        "            'revenue_perdido': revenue_perdido,\n",
        "            'revenue_total': total_revenue\n",
        "        }\n",
        "    else:\n",
        "        print(\"No se puede calcular impacto económico sin datos de churn y facturación\")\n",
        "        return None\n",
        "\n",
        "metricas_negocio = calcular_impacto_negocio(df_clean, churn_column)\n"
      ],
      "id": "OrjdRDIrbrjA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVIJK2onbrjA"
      },
      "source": [
        "## 12. Exportación de resultados\n"
      ],
      "id": "fVIJK2onbrjA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoefhokNbrjB"
      },
      "outputs": [],
      "source": [
        "def exportar_resultados_completos(df, insights, hallazgos, recomendaciones, cols_numericas):\n",
        "    try:\n",
        "        df.to_csv('telecom_churn_data_clean.csv', index=False, encoding='utf-8')\n",
        "        print(\"Dataset limpio exportado: telecom_churn_data_clean.csv\")\n",
        "        with open('telecom_churn_executive_report.txt', 'w', encoding='utf-8') as f:\n",
        "            f.write(\"TELECOM X - REPORTE EJECUTIVO DE CHURN\\n\")\n",
        "            f.write(\"=\"*50 + \"\\n\\n\")\n",
        "            f.write(f\"Fecha: {datetime.now().strftime('%d/%m/%Y %H:%M')}\\n\")\n",
        "            f.write(f\"Dataset: {len(df)} clientes analizados\\n\\n\")\n",
        "            f.write(\"INSIGHTS PRINCIPALES:\\n\")\n",
        "            f.write(\"-\"*25 + \"\\n\")\n",
        "            for i, insight in enumerate(insights, 1):\n",
        "                f.write(f\"{i}. {insight}\\n\")\n",
        "            f.write(\"\\nHALLAZGOS CLAVE:\\n\")\n",
        "            f.write(\"-\"*20 + \"\\n\")\n",
        "            for i, hallazgo in enumerate(hallazgos, 1):\n",
        "                f.write(f\"{i}. {hallazgo}\\n\")\n",
        "            f.write(\"\\nRECOMENDACIONES:\\n\")\n",
        "            f.write(\"-\"*20 + \"\\n\")\n",
        "            for i, rec in enumerate(recomendaciones, 1):\n",
        "                f.write(f\"{i}. {rec}\\n\")\n",
        "        print(\"Reporte ejecutivo exportado: telecom_churn_executive_report.txt\")\n",
        "        if cols_numericas:\n",
        "            estadisticas = df[cols_numericas].describe()\n",
        "            estadisticas.to_csv('telecom_churn_descriptive_stats.csv', encoding='utf-8')\n",
        "            print(\"Estadísticas descriptivas exportadas: telecom_churn_descriptive_stats.csv\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error en exportación: {e}\")\n",
        "\n",
        "exportar_resultados_completos(df_clean, insights_avanzados, hallazgos_final, recomendaciones_final, cols_numericas)\n"
      ],
      "id": "yoefhokNbrjB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LVYqa1LbrjB"
      },
      "source": [
        "## 13. Resumen final\n"
      ],
      "id": "6LVYqa1LbrjB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijaFghZhbrjB"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ANÁLISIS COMPLETADO EXITOSAMENTE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Clientes analizados: {len(df_clean):,}\")\n",
        "print(f\"Variables procesadas: {len(df_clean.columns)}\")\n",
        "calidad = (1 - df_clean.isnull().sum().sum() / (len(df_clean) * len(df_clean.columns))) * 100\n",
        "print(f\"Calidad de datos: {calidad:.1f}%\")\n",
        "if churn_column:\n",
        "    try:\n",
        "        if pd.api.types.is_numeric_dtype(df_clean[churn_column]):\n",
        "            tasa_final = df_clean[churn_column].mean()\n",
        "        else:\n",
        "            vals = df_clean[churn_column].dropna().unique()\n",
        "            yes_alias = {'yes', 'sí', 'si', '1', 'true'}\n",
        "            val_churn = None\n",
        "            for v in vals:\n",
        "                if str(v).strip().lower() in yes_alias:\n",
        "                    val_churn = v\n",
        "                    break\n",
        "            if val_churn is None and len(vals) > 0:\n",
        "                val_churn = vals[0]\n",
        "            tasa_final = (df_clean[churn_column] == val_churn).mean()\n",
        "        print(f\"Tasa de churn identificada: {tasa_final:.1%}\")\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo calcular tasa final de churn: {e}\")\n",
        "if metricas_negocio:\n",
        "    print(f\"Revenue en riesgo (estimado): ${metricas_negocio['revenue_perdido']:,.2f}\")\n",
        "print(f\"\\nAnálisis finalizado: {datetime.now().strftime('%d/%m/%Y a las %H:%M')}\")\n",
        "print(\"Archivos generados:\")\n",
        "print(\"  - telecom_churn_data_clean.csv\")\n",
        "print(\"  - telecom_churn_executive_report.txt\")\n",
        "print(\"  - telecom_churn_descriptive_stats.csv\")\n",
        "print(\"\\nProyecto listo para:\")\n",
        "print(\"  * Implementación de modelos de ML\")\n",
        "print(\"  * Desarrollo de estrategias de retención\")\n",
        "print(\"  * Presentación a stakeholders\")\n",
        "print(\"  * Despliegue en producción\")\n"
      ],
      "id": "ijaFghZhbrjB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg7aje9abrjB"
      },
      "source": [
        "## 14. Manejo de error de carga (si falla la API)\n"
      ],
      "id": "Lg7aje9abrjB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PhDP92dbrjB"
      },
      "outputs": [],
      "source": [
        "if df_raw is None:\n",
        "    print(\"\\nERROR CRÍTICO\")\n",
        "    print(\"No se pudieron cargar los datos desde la API.\")\n",
        "    print(\"Verificar:\")\n",
        "    print(\" - Conexión a internet\")\n",
        "    print(\" - Disponibilidad de la API\")\n",
        "    print(\" - URL correcta del endpoint\")\n"
      ],
      "id": "0PhDP92dbrjB"
    }
  ]
}